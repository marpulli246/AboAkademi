{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_AnalysisFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-a__QLe74qY"
      },
      "source": [
        "# Sentiment Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CGUXJnb74qj",
        "collapsed": true,
        "outputId": "e7d488e0-8688-4dc5-a0c8-f8a42d268af3"
      },
      "source": [
        "#Import all the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Input, Flatten, Conv1D, LSTM, GRU, Bidirectional\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rtwP6el74ql"
      },
      "source": [
        "#Read the data file. If tun in Jupyter use the other code line.\n",
        "tweets = pd.read_csv('/content/Sentiment140.tenPercent.sample.tweets.tsv',sep='\\t')\n",
        "#tweets = pd.read_csv('Sentiment140.tenPercent.sample.tweets.tsv',sep='\\t') Use with Jupyter\n",
        "\n",
        "#tweets.isnull().values.any()\n",
        "#tweets.shape"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvJ96V6R74qo"
      },
      "source": [
        "#Have a look of the data.Sentiment labe 0 and 4. No neutrals (2).\r\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_I5pEkYZncE"
      },
      "source": [
        "#Data types and if anything missing.\r\n",
        "tweets.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfZlyLG574qr"
      },
      "source": [
        "#Check one sample data.\n",
        "tweets[\"tweet_text\"][1200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCGK8yHT74qs"
      },
      "source": [
        "#How balanced is the sentiment data.\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(x='sentiment_label', data=tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSEFyS8z74qu"
      },
      "source": [
        "#Removing all the unnecessary characters.\n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Removing quotes\n",
        "    sentence = re.sub('quot', '', sentence)\n",
        "    #sentence = sentence.strip('quot')\n",
        "    # Removing amps\n",
        "    sentence = re.sub('amp', '', sentence)\n",
        "    #sentence = sentence.strip('amp')\n",
        "\n",
        "    #sentence = re.sub('@[^\\s]+','',sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt3M3xiQ74qw"
      },
      "source": [
        "#Call the function to remove unwanted characters.\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmx-iviE74qy"
      },
      "source": [
        "#Create the features variable array.\n",
        "X = []\n",
        "sentences = list(tweets['tweet_text'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81pOkgnnxcLz"
      },
      "source": [
        "#Check for most common wording.\r\n",
        "import collections\r\n",
        "import itertools\r\n",
        "# Create counter\r\n",
        "\r\n",
        "# List of all words across tweets\r\n",
        "#all_words_no_urls = list(itertools.chain(*X))\r\n",
        "counts_no_urls = collections.Counter(X)\r\n",
        "counts_no_urls.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrtFUeV8y4oB"
      },
      "source": [
        "#Show most common wording on a bar plot.\r\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\r\n",
        "clean_tweets_no_urls = pd.DataFrame(counts_no_urls.most_common(20),\r\n",
        "                             columns=['words', 'count'])\r\n",
        "# Plot horizontal bar graph\r\n",
        "clean_tweets_no_urls.sort_values(by='count').plot.barh(x='words',\r\n",
        "                      y='count',\r\n",
        "                      ax=ax,\r\n",
        "                      color=\"blue\")\r\n",
        "\r\n",
        "ax.set_title(\"Common Words Found in Tweets (Including All Words)\")\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X9W8R93w1ZE"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS \r\n",
        "\r\n",
        "comment_words = '' \r\n",
        "stopwords = set(STOPWORDS) \r\n",
        "\r\n",
        "# iterate through the csv file \r\n",
        "all_tweets = pd.DataFrame(tweets)\r\n",
        "for val in X: \r\n",
        "      \r\n",
        "    # typecaste each val to string \r\n",
        "    val = str(val) \r\n",
        "  \r\n",
        "    # split the value \r\n",
        "    tokens = val.split() \r\n",
        "      \r\n",
        "    # Converts each token into lowercase \r\n",
        "    for i in range(len(tokens)): \r\n",
        "        tokens[i] = tokens[i].lower() \r\n",
        "      \r\n",
        "    comment_words += \" \".join(tokens)+\" \"\r\n",
        "\r\n",
        "wordcloud = WordCloud(width = 1600, height = 1200,\r\n",
        "                max_words = 200000,       \r\n",
        "                #background_color ='white', \r\n",
        "                stopwords = stopwords,\r\n",
        "                colormap = 'Dark2', \r\n",
        "                min_font_size = 10).generate(comment_words) \r\n",
        "  \r\n",
        "# plot the WordCloud image                        \r\n",
        "plt.figure(figsize = (15, 15), facecolor = None) \r\n",
        "plt.imshow(wordcloud) \r\n",
        "plt.axis(\"off\") \r\n",
        "plt.tight_layout(pad = 0) \r\n",
        "  \r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcow74Y474qz"
      },
      "source": [
        "#Retrieve the label information.\n",
        "y = np.array(tweets['sentiment_label'])\n",
        "y = np.array(list(map(lambda x: 0 if x==0 else 1, y)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0RhrsDP74q3"
      },
      "source": [
        "#Split the data to training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seLPs3VS6BYc"
      },
      "source": [
        "#DO NOT RUN. Removing stopwords. Takes LOOONG time to run it.Code can be completed without this. Not a huge impact on results.\r\n",
        "print(stopwords.words('english'))\r\n",
        "\r\n",
        "X_train = [word for word in X_train if not word in stopwords.words()]\r\n",
        "X_test = [word for word in X_test if not word in stopwords.words()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-NaSyNZ74q3"
      },
      "source": [
        "#Tokenize both training and test sets.\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCZ3gqFnOuhI"
      },
      "source": [
        "#Check how the data looks.\r\n",
        "X_train[1202]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2TRztVo74q3"
      },
      "source": [
        "# Do the padding.\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "#vocab_size = len(tokenizer.word_index)\n",
        "\n",
        "maxlen = 50\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='pre' , maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQUZiFO74q4"
      },
      "source": [
        "#Start enbedding the words using GloVe pre-trained word vectors.Use the commented line in case running this code in Jupyter.\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/glove.6B.50d.txt',encoding=\"utf8\") \n",
        "#glove_file = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssKJySAv74q5"
      },
      "source": [
        "#Complete embedding.\n",
        "embedding_matrix = zeros((vocab_size, 50))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Hzg156g83p"
      },
      "source": [
        "#See how enbedding looks.\r\n",
        "print(embedding_matrix.shape)\r\n",
        "plt.plot(embedding_matrix[5])\r\n",
        "plt.plot(embedding_matrix[10])\r\n",
        "plt.plot(embedding_matrix[20])\r\n",
        "plt.plot(embedding_matrix[1000])\r\n",
        "plt.title('Embedding Vectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB0f_pDR74rK"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrvIbESzYOcD"
      },
      "source": [
        "#Build the network. layers are embedding, flatten and dense. See the vizualized layout at the bottom.\r\n",
        "#Compile the model with optimize type, loss method and KPIs.\r\n",
        "\r\n",
        "from tensorflow.keras import layers, optimizers, losses\r\n",
        "\r\n",
        "model = keras.Sequential([                     \r\n",
        "    layers.Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=maxlen),\r\n",
        "    layers.Dropout(0.2),\r\n",
        "    layers.Conv1D(128, 3, activation='relu'),\r\n",
        "    layers.Dropout(0.2),\r\n",
        "    layers.MaxPool1D(3),\r\n",
        "    layers.Dropout(0.2),\r\n",
        "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\r\n",
        "    layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4lOHUv74rO"
      },
      "source": [
        "#Fit the model using hyperparameters.Save the theperformance data for further analysis.\n",
        "history_RNN = model.fit(X_train, y_train, batch_size=64, epochs=6, verbose=1, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPGLp3moif_f",
        "outputId": "8bbfd6f3-22fe-4e5e-b198-0103d0ad738b"
      },
      "source": [
        "#Check the accuracy.\r\n",
        "score = model.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 21s 14ms/step - loss: 0.4485 - acc: 0.7911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHLH2Vad74rO",
        "outputId": "3b4c5065-6fff-469f-8f79-52c8b7d1db31"
      },
      "source": [
        "#Print final model test performance.\n",
        "print(\"Test Loss:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.44850629568099976\n",
            "Test Accuracy: 0.7910742163658142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T_CzHbeIShu"
      },
      "source": [
        "#Have a look on prediction data.\r\n",
        "prediction = model.predict(X_test)\r\n",
        "prediction[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57RK_WCD74rO"
      },
      "source": [
        "#Plot the data for analysis.\n",
        "plt.plot(history_RNN.history['acc'])\n",
        "plt.plot(history_RNN.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_RNN.history['loss'])\n",
        "plt.plot(history_RNN.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ickjCAARDaem"
      },
      "source": [
        "## RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPxa6sqwDgzH"
      },
      "source": [
        "#Print RF classifier prediction results for all the rounds training is done.\r\n",
        "def print_results(results):\r\n",
        "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\r\n",
        "    means = results.cv_results_['mean_test_score']\r\n",
        "    stds = results.cv_results_['std_test_score']\r\n",
        "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\r\n",
        "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSb9kvG30755"
      },
      "source": [
        "#Run RF training for multiple parameter settings. Takes pretty LONG time to run as max depth is not limited.\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "rf = RandomForestClassifier()\r\n",
        "parameters = {\r\n",
        "    'n_estimators': [50, 100, 200, 250],\r\n",
        "    'max_depth': [20, None]\r\n",
        "}\r\n",
        "\r\n",
        "cv = GridSearchCV(rf, parameters, cv=5)\r\n",
        "cv.fit(X_train, y_train)\r\n",
        "\r\n",
        "print_results(cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnaCmKkaHwgT"
      },
      "source": [
        "#Check three best models once again for final best fit.\r\n",
        "rf1 = RandomForestClassifier(n_estimators=200, max_depth=20)\r\n",
        "rf1.fit(X_train, y_train)\r\n",
        "\r\n",
        "rf2 = RandomForestClassifier(n_estimators=200, max_depth=None)\r\n",
        "rf2.fit(X_train, y_train)\r\n",
        "\r\n",
        "rf3 = RandomForestClassifier(n_estimators=250, max_depth=None)\r\n",
        "rf3.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D270eNVuH0LX"
      },
      "source": [
        "#Print results\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\r\n",
        "\r\n",
        "for mdl in [rf1, rf2, rf3]:\r\n",
        "    y_pred = mdl.predict(X_test)\r\n",
        "    accuracy = round(accuracy_score(y_test, y_pred), 3)\r\n",
        "    precision = round(precision_score(y_test, y_pred), 3)\r\n",
        "    recall = round(recall_score(y_test, y_pred), 3)\r\n",
        "    print('MAX DEPTH: {} / # OF EST: {} -- Accuracy: {} / Presicion: {} / Recall: {}'.format(mdl.max_depth,\r\n",
        "                                                                         mdl.n_estimators,\r\n",
        "                                                                         accuracy,\r\n",
        "                                                                         precision,\r\n",
        "                                                                         recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpo7QbtVDgrk"
      },
      "source": [
        "#Confusion matrix\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "print(confusion_matrix(y_test,y_pred))\r\n",
        "print(classification_report(y_test,y_pred))\r\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRBeCz53SpR9"
      },
      "source": [
        "## **Network Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iLBvNyt5S3G"
      },
      "source": [
        "#Import libraries to enable network architecture visualization.\r\n",
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FrwaCVu5VvM"
      },
      "source": [
        "#Visualize network architecture\r\n",
        "SVG(model_to_dot(model, dpi=64, show_shapes=True).create(prog=\"dot\", format=\"svg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ncNTlaCR0Zi"
      },
      "source": [
        "#Visualize the random forest tree.\r\n",
        "from sklearn import tree\r\n",
        "len(rf1.estimators_)\r\n",
        "feat = pd.DataFrame(X_train)\r\n",
        "plt.figure(figsize=(20,20))\r\n",
        "_ = tree.plot_tree(rf1.estimators_[0], feature_names=feat.columns, filled=True, max_depth =3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}